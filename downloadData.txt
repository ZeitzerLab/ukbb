#!/bin/bash
#SBATCH --time=1:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=16
#SBATCH --account=laraweed.guest
#SBATCH --partition=jzeitzer.guest
#SBATCH -O qe-%j
#SBATCH --mem-per-cpu=1
#SBATCH --job-name=test
#SBATCH --mem-per-cpu=1


# Load modules
module purge
ml python/3.6.1
ml py-numpy/1.18.1_py36
ml py-scipy/1.4.1_py36
ml py-scikit-learn/0.24.2_py36
ml py-pandas/1.0.3_py36


# Set file paths
DATARAW = /scratch/groups/jzeitzer/UKBB/Data/Raw


# Make sure directories exist
mkdir -p $DATARAW

cd $DATARAW



# Copy .cwa data to DATARAW from UKBB website or wherever it's stored
# Ask renske for code to do this
ukbfetch -binput.txt -ak63099r43661.ukbkey
# make sure the key and bulk files are there
# input.txt also used to determine what files to pull

# Submit next job
# generate commands
j1 =$(sbatch  --mem=3g --cpus-per-task=4 job1.py)
j2 =$(sbatch  --dependency=afterok:$j1 --mem=3g --cpus-per-task=4 job2.py)
j3 =$(sbatch  --dependency=afterok:$j2 --mem=3g --cpus-per-task=4 job3.py) 
The first cose line would send in the first job normally with a memory allocation of 3Gb and 4 CPUs.

